{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable Graph Anomaly Detection - Exploratory Analysis\n",
    "\n",
    "This notebook provides exploratory analysis and visualization capabilities for the GAD pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.config import load_config\n",
    "from models.models import *\n",
    "from utils.evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_name = 'weibo'  # Change as needed\n",
    "data_path = f'../data/processed/{dataset_name}_static.pt'\n",
    "\n",
    "if Path(data_path).exists():\n",
    "    data = torch.load(data_path, map_location='cpu')\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Nodes: {data.x.shape[0]}\")\n",
    "    print(f\"Features: {data.x.shape[1]}\")\n",
    "    print(f\"Edges: {data.edge_index.shape[1]}\")\n",
    "    print(f\"Anomaly ratio: {data.y.float().mean():.4f}\")\n",
    "else:\n",
    "    print(f\"Dataset {dataset_name} not found. Available datasets in data/processed/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature distributions\n",
    "if 'data' in locals():\n",
    "    # Plot feature statistics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Feature mean distribution\n",
    "    axes[0,0].hist(data.x.mean(dim=0).numpy(), bins=50, alpha=0.7)\n",
    "    axes[0,0].set_title('Feature Mean Distribution')\n",
    "    axes[0,0].set_xlabel('Mean Value')\n",
    "    \n",
    "    # Feature std distribution\n",
    "    axes[0,1].hist(data.x.std(dim=0).numpy(), bins=50, alpha=0.7)\n",
    "    axes[0,1].set_title('Feature Std Distribution')\n",
    "    axes[0,1].set_xlabel('Std Value')\n",
    "    \n",
    "    # Node degree distribution\n",
    "    from torch_geometric.utils import degree\n",
    "    degrees = degree(data.edge_index[0], data.x.shape[0])\n",
    "    axes[1,0].hist(degrees.numpy(), bins=50, alpha=0.7)\n",
    "    axes[1,0].set_title('Node Degree Distribution')\n",
    "    axes[1,0].set_xlabel('Degree')\n",
    "    axes[1,0].set_yscale('log')\n",
    "    \n",
    "    # Anomaly vs normal feature comparison\n",
    "    normal_features = data.x[data.y == 0].mean(dim=0)\n",
    "    anomaly_features = data.x[data.y == 1].mean(dim=0)\n",
    "    \n",
    "    feature_diff = (anomaly_features - normal_features).abs()\n",
    "    top_discriminative = torch.topk(feature_diff, 10)\n",
    "    \n",
    "    axes[1,1].bar(range(10), top_discriminative.values.numpy())\n",
    "    axes[1,1].set_title('Top 10 Discriminative Features')\n",
    "    axes[1,1].set_xlabel('Feature Index')\n",
    "    axes[1,1].set_ylabel('Mean Difference')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple anomaly detection example\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "if 'data' in locals():\n",
    "    # Prepare data\n",
    "    X = data.x.numpy()\n",
    "    y = data.y.numpy()\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Simple L2 distance-based anomaly detection\n",
    "    center = X_scaled.mean(axis=0)\n",
    "    distances = np.linalg.norm(X_scaled - center, axis=1)\n",
    "    \n",
    "    # Evaluate\n",
    "    roc_auc = roc_auc_score(y, distances)\n",
    "    avg_precision = average_precision_score(y, distances)\n",
    "    \n",
    "    print(f\"L2 Distance Method:\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(distances[y==0], bins=50, alpha=0.7, label='Normal', density=True)\n",
    "    plt.hist(distances[y==1], bins=50, alpha=0.7, label='Anomaly', density=True)\n",
    "    plt.xlabel('L2 Distance from Center')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.title('Distance Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y, distances)\n",
    "    plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.title('ROC Curve')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example explainability analysis\n",
    "if 'data' in locals():\n",
    "    # Find most anomalous nodes\n",
    "    anomaly_scores = distances\n",
    "    top_anomalies = np.argsort(anomaly_scores)[-10:]\n",
    "    \n",
    "    print(\"Top 10 Most Anomalous Nodes:\")\n",
    "    for i, node_idx in enumerate(top_anomalies):\n",
    "        score = anomaly_scores[node_idx]\n",
    "        true_label = y[node_idx]\n",
    "        print(f\"{i+1:2d}. Node {node_idx:5d}: Score={score:.4f}, True Label={'Anomaly' if true_label else 'Normal'}\")\n",
    "    \n",
    "    print(f\"\\nAccuracy on top anomalies: {y[top_anomalies].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_comprehensive(dataset_name):\n",
    "    \"\"\"Comprehensive analysis of a dataset\"\"\"\n",
    "    data_path = f'../data/processed/{dataset_name}_static.pt'\n",
    "    \n",
    "    if not Path(data_path).exists():\n",
    "        print(f\"Dataset {dataset_name} not found\")\n",
    "        return\n",
    "    \n",
    "    data = torch.load(data_path, map_location='cpu')\n",
    "    \n",
    "    print(f\"\\n=== {dataset_name.upper()} Dataset Analysis ===\")\n",
    "    print(f\"Nodes: {data.x.shape[0]:,}\")\n",
    "    print(f\"Features: {data.x.shape[1]:,}\")\n",
    "    print(f\"Edges: {data.edge_index.shape[1]:,}\")\n",
    "    print(f\"Anomaly ratio: {data.y.float().mean():.4f} ({data.y.sum().item():,} anomalies)\")\n",
    "    \n",
    "    # Graph connectivity\n",
    "    from torch_geometric.utils import degree\n",
    "    degrees = degree(data.edge_index[0], data.x.shape[0])\n",
    "    print(f\"Average degree: {degrees.mean().item():.2f}\")\n",
    "    print(f\"Max degree: {degrees.max().item()}\")\n",
    "    \n",
    "    # Feature statistics\n",
    "    print(f\"Feature range: [{data.x.min().item():.4f}, {data.x.max().item():.4f}]\")\n",
    "    print(f\"Feature mean: {data.x.mean().item():.4f}\")\n",
    "    print(f\"Feature std: {data.x.std().item():.4f}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Analyze all available datasets\n",
    "datasets = ['weibo', 'amazon', 'elliptic', 'tfinance', 'yelp']\n",
    "for dataset in datasets:\n",
    "    try:\n",
    "        analyze_dataset_comprehensive(dataset)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {dataset}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}